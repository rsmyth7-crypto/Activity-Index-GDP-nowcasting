# -*- coding: utf-8 -*-
"""big-data-project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/gist/rsmyth7-crypto/54c74b095085b2a4b881325695bf9022/big-data-project.ipynb

# Create Folders
"""

import os

project_root = "/content/economic_search_project"

folders = [
    "data/raw",
    "data/processed",
    "analysis",
    "notebooks",
    "results",
    "models",
    "docs"
]

for folder in folders:
    os.makedirs(os.path.join(project_root, folder), exist_ok=True)

project_root

"""# Gather Data

### **Upload quarterly GDP**
"""

import os
from google.colab import files
import shutil

# Upload the file
uploaded = files.upload()

# Get the name of the uploaded file
for filename in uploaded.keys():
    uploaded_filename = filename
    print(f'User uploaded file "{uploaded_filename}" with length {len(uploaded[filename])} bytes')

project_root = '/content/economic_search_project' # Assuming project_root is defined as in the previous cell
target_folder = os.path.join(project_root, 'data/raw')

# Move the uploaded file to the target folder
if 'uploaded_filename' in locals():
    destination_path = os.path.join(target_folder, uploaded_filename)
    shutil.move(uploaded_filename, destination_path)
    print(f'Moved "{uploaded_filename}" to "{destination_path}"')
else:
    print("No file was uploaded.")

"""### **Collecting data on google search trends using hospitality key words and saving as CSV file in raw data folder**




"""

!pip install pytrends
from pytrends.request import TrendReq
import pandas as pd
import os

pytrends = TrendReq()

keywords = ["hotels", "restaurants", "concert tickets", "gigs", "pubs"]
pytrends.build_payload(keywords, geo='IE', timeframe='2010-01-01 2025-01-01')

df_trends = pytrends.interest_over_time()

project_root = '/content/economic_search_project' # Ensure project_root is defined or accessible
output_path = os.path.join(project_root, 'data/raw', 'google_trends.csv')
df_trends.to_csv(output_path)

"""Increase number of key words

### **Aggregate the google trends into quarterly format**
"""

df_quarterly = df_trends.resample('Q').mean()

"""### **Save data in data/processed folder**"""

import os

output_path_processed = os.path.join(project_root, 'data/processed', 'quarterly_google_trends.csv')
df_quarterly.to_csv(output_path_processed)

print(f"Quarterly aggregated trends saved to: {output_path_processed}")

"""### **Merge the two data sets**"""

import pandas as pd

# Load the GDP data from the uploaded CSV
gdp_df = pd.read_csv(destination_path)

# Process the GDP data:
# Convert 'Quarter' column to datetime, setting it as the index
gdp_df['Quarter'] = pd.PeriodIndex(gdp_df['Quarter'], freq='Q').to_timestamp(how='end').normalize()
gdp_df = gdp_df.set_index('Quarter')

# Select the 'VALUE' column and rename it to 'GDP'
gdp_df = gdp_df[['VALUE']]
gdp_df.rename(columns={'VALUE': 'GDP'}, inplace=True)

# Rename the index to 'date' to match the index name of df_quarterly for a clean merge
gdp_df.index.name = 'date'

# Merge the two dataframes
df = pd.merge(gdp_df, df_quarterly, left_index=True, right_index=True)
print("Merged DataFrame using pd.merge():")
print(pd.merge)

"""### **Create hospitality activity index using Google trends key words**"""

from sklearn.decomposition import PCA

pca = PCA(n_components=1)
df['hospitality_index'] = pca.fit_transform(df_quarterly[keywords])

"""# Investigate Correlations

###**The correlations between the individual search terms and GDP**
"""

import seaborn as sns
import matplotlib.pyplot as plt

# Calculate the correlation matrix
correlation_matrix = df.corr(numeric_only=True)

# Display the correlation matrix
print("Correlation Matrix:")
display(correlation_matrix)

# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of GDP and Google Trends Data')
plt.show()

"""###**Correlation matrix for past five years**"""

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Determine the start date for the last five years
five_years_ago = df.index.max() - pd.DateOffset(years=5)

# Filter the DataFrame for the last five years
df_last_5_years = df[df.index >= five_years_ago]

# Select relevant columns for correlation (GDP and keywords)
correlation_columns = ['GDP'] + keywords
df_corr_subset = df_last_5_years[correlation_columns]

# Calculate the correlation matrix for the last 5 years
correlation_matrix_5_years = df_corr_subset.corr(numeric_only=True)

# Display the correlation matrix
print("Correlation Matrix for Last 5 Years:")
display(correlation_matrix_5_years)

# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix_5_years, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of GDP and Google Trends Data (Last 5 Years)')
plt.show()

"""###**Saving the correlation matrices to results folder**"""

import os

results_folder = os.path.join(project_root, 'results')

# Save the full correlation matrix
output_path_full_corr = os.path.join(results_folder, 'full_correlation_matrix.csv')
correlation_matrix.to_csv(output_path_full_corr)
print(f"Full correlation matrix saved to: {output_path_full_corr}")

# Save the 5-year correlation matrix
output_path_5yr_corr = os.path.join(results_folder, 'five_year_correlation_matrix.csv')
correlation_matrix_5_years.to_csv(output_path_5yr_corr)
print(f"5-year correlation matrix saved to: {output_path_5yr_corr}")

"""# OLS Regression

###**Run overall OLS regression**
"""

import statsmodels.api as sm

# Define the independent variable (hospitality_index) and add a constant for the full dataset
X_full = sm.add_constant(df[['hospitality_index']])

# Define the dependent variable (GDP) for the full dataset
y_full = df['GDP']

# Run the OLS regression model for the full dataset
model_full = sm.OLS(y_full, X_full).fit()

# Print the summary of the regression results
print(model_full.summary())

"""###**OLS for past five years**"""

import statsmodels.api as sm
import pandas as pd

# Determine the start date for the last five years
five_years_ago = df.index.max() - pd.DateOffset(years=5)

# Filter the DataFrame for the last five years
df_last_5_years = df[df.index >= five_years_ago]

# Define the independent variable (hospitality_index) and add a constant
X_5_years = sm.add_constant(df_last_5_years[['hospitality_index']])

# Define the dependent variable (GDP)
y_5_years = df_last_5_years['GDP']

# Run the OLS regression model for the last five years
model_5_years = sm.OLS(y_5_years, X_5_years).fit()

# Print the summary of the regression results
print(model_5_years.summary())

"""###**Evaluate the regression of the lagges GDP and Hospitality index**"""

importances = rf.feature_importances_
feature_names = X.columns

for name, imp in zip(feature_names, importances):
    print(name, ":", imp)

"""###**Save regression tables and interpretations to files**"""

import os

results_folder = os.path.join(project_root, 'results')
os.makedirs(results_folder, exist_ok=True)

# Prepare the interpretation for the full dataset model
interpretation_full = f"\n\nOLS INTERPRETATION (Full Dataset):\n"
interpretation_full += f"R-squared: {model_full.rsquared:.3f}\n"
interpretation_full += f"Coefficient on hospitality_index: {model_full.params['hospitality_index']:.2f}\n"
if model_full.pvalues['hospitality_index'] < 0.05:
    interpretation_full += "→ Statistically significant.\n"
else:
    interpretation_full += "→ Not statistically significant.\n"

# Save the summary of the full dataset regression model and its interpretation
summary_full_path = os.path.join(results_folder, 'ols_summary_full_dataset.txt')
with open(summary_full_path, 'w') as f:
    f.write(model_full.summary().as_text())
    f.write(interpretation_full)
print(f"Full dataset OLS regression summary and interpretation saved to: {summary_full_path}")

# Prepare the interpretation for the 5-year model
interpretation_5_years = f"\n\nOLS INTERPRETATION (Last 5 Years):\n"
interpretation_5_years += f"R-squared: {model_5_years.rsquared:.3f}\n"
interpretation_5_years += f"Coefficient on hospitality_index: {model_5_years.params['hospitality_index']:.2f}\n"
if model_5_years.pvalues['hospitality_index'] < 0.05:
    interpretation_5_years += "→ Statistically significant.\n"
else:
    interpretation_5_years += "→ Not statistically significant.\n"

# Save the summary of the 5-year regression model and its interpretation
summary_5_years_path = os.path.join(results_folder, 'ols_summary_5_years.txt')
with open(summary_5_years_path, 'w') as f:
    f.write(model_5_years.summary().as_text())
    f.write(interpretation_5_years)
print(f"5-year OLS regression summary and interpretation saved to: {summary_5_years_path}")

"""# LASSO regression

###LASSO regression for full data set
"""

from sklearn.linear_model import LassoCV
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np
import os

# ================q
# 1. Prepare data
# ================q

# Create lagged GDP and GDP growth features
df['GDP_lag1'] = df['GDP'].shift(1)
df['GDP_growth'] = df['GDP'].pct_change()

# Drop rows with NaN values that result from lagging/pct_change
df_model = df.dropna(subset=['GDP_lag1', 'GDP_growth', 'hospitality_index'])

X = df_model[['GDP_lag1', 'hospitality_index']]
y = df_model['GDP_growth']

# Scale predictors (LASSO requires scaling)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ===================
# 2. Run LASSO with CV
# ===================
lasso = LassoCV(cv=5, random_state=42)
lasso.fit(X_scaled, y)

# =========================
# 3. Print model information
# =========================
print("Best alpha (λ):", lasso.alpha_)
print("\nCoefficients:")

coef_table = pd.DataFrame({
    'feature': X.columns,
    'coefficient': lasso.coef_
})

print(coef_table)

# Identify which features LASSO kept
selected_features = coef_table[coef_table['coefficient'] != 0]['feature'].tolist()
zeroed_features = coef_table[coef_table['coefficient'] == 0]['feature'].tolist()

print("\nSelected features (non-zero):", selected_features)
print("Zeroed-out features:", zeroed_features)

# ======================
# 4. Save results
# ======================
results_folder = os.path.join(project_root, 'results')
os.makedirs(results_folder, exist_ok=True)

coef_table.to_csv(os.path.join(results_folder, "lasso_coefficients.csv"))
print("\nSaved coefficient table to /results folder.")

"""### Lasso Regression - Trained on Last 5 Years of Data"""

from sklearn.linear_model import LassoCV
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np
import os

# Ensure df has the latest lagged and growth features
df['GDP_lag1'] = df['GDP'].shift(1)
df['GDP_growth'] = df['GDP'].pct_change()

# Drop rows with NaN values that result from lagging/pct_change for the full dataset
df_model_full = df.dropna(subset=['GDP_lag1', 'GDP_growth', 'hospitality_index'])

# Filter df_model for only the last five years
five_years_ago_lasso = df_model_full.index.max() - pd.DateOffset(years=5)
df_model_5_years_lasso = df_model_full[df_model_full.index >= five_years_ago_lasso]

# ================q
# 1. Prepare data
# ================q
X_lasso_5_years = df_model_5_years_lasso[['GDP_lag1', 'hospitality_index']]
y_lasso_5_years = df_model_5_years_lasso['GDP_growth']

# Scale predictors (LASSO requires scaling)
scaler_lasso_5_years = StandardScaler()
X_scaled_lasso_5_years = scaler_lasso_5_years.fit_transform(X_lasso_5_years)

# ===================
# 2. Run LASSO with CV
# ===================
lasso_5_years = LassoCV(cv=5, random_state=42)
lasso_5_years.fit(X_scaled_lasso_5_years, y_lasso_5_years)

# =========================
# 3. Print model information
# =========================
print("Best alpha (λ) for 5-year Lasso:", lasso_5_years.alpha_)
print("\nCoefficients for 5-year Lasso:")

coef_table_5_years = pd.DataFrame({
    'feature': X_lasso_5_years.columns,
    'coefficient': lasso_5_years.coef_
})

print(coef_table_5_years)

# Identify which features LASSO kept
selected_features_lasso_5_years = coef_table_5_years[coef_table_5_years['coefficient'] != 0]['feature'].tolist()
zeroed_features_lasso_5_years = coef_table_5_years[coef_table_5_years['coefficient'] == 0]['feature'].tolist()

print("\nSelected features (non-zero) for 5-year Lasso:", selected_features_lasso_5_years)
print("Zeroed-out features for 5-year Lasso:", zeroed_features_lasso_5_years)

# ======================
# 4. Save results
# ======================
results_folder = os.path.join(project_root, 'results')
coef_table_5_years.to_csv(os.path.join(results_folder, "lasso_coefficients_5_years.csv"))
print("\nSaved 5-year Lasso coefficient table to /results folder.")

"""# Random Forest Regression

### **Random Forest regression for whole data set**
"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error
import numpy as np
import pandas as pd # Import pandas for DataFrame creation

# Create lagged GDP and GDP growth features
df['GDP_lag1'] = df['GDP'].shift(1)
df['GDP_growth'] = df['GDP'].pct_change()

# Drop rows with NaN values that result from lagging/pct_change
df_model = df.dropna(subset=['GDP_lag1', 'GDP_growth', 'hospitality_index'])

X = df_model[['GDP_lag1', 'hospitality_index']]
y = df_model['GDP_growth']

tscv = TimeSeriesSplit(n_splits=5)

all_preds = []
all_actuals = []
all_dates = [] # To store dates for each prediction

for train_idx, test_idx in tscv.split(X):
    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

    rf = RandomForestRegressor(n_estimators=500, random_state=42)
    rf.fit(X_train, y_train)

    pred = rf.predict(X_test)
    all_preds.extend(pred)
    all_actuals.extend(y_test.tolist()) # Convert Series to list
    all_dates.extend(y_test.index.tolist()) # Store dates

# Create a DataFrame for easier plotting and filtering
nowcast_results_df = pd.DataFrame({
    'date': all_dates,
    'actual': all_actuals,
    'predicted': all_preds
}).set_index('date').sort_index()

rmse = np.sqrt(mean_squared_error(all_actuals, all_preds))
print("RF RMSE:", rmse)

"""###**Create graph for Full time period of nowcasting**"""

import matplotlib.pyplot as plt
import pandas as pd

plt.figure(figsize=(12, 6))
plt.plot(nowcast_results_df.index, nowcast_results_df['actual'], label="Actual GDP Growth")
plt.plot(nowcast_results_df.index, nowcast_results_df['predicted'], label="Predicted GDP Growth")
plt.legend()
plt.title("Nowcast vs Actual GDP Growth (Full Dataset)")
plt.xlabel("Date")
plt.ylabel("GDP Growth")
plt.grid(True)
plt.show()

"""###**Now cast for the past five years**"""

import matplotlib.pyplot as plt
import pandas as pd

# Determine the start date for the last five years
# Assuming the latest date in your data is nowcast_results_df.index.max()
five_years_ago = nowcast_results_df.index.max() - pd.DateOffset(years=5)

# Filter the DataFrame for the last five years
filtered_results = nowcast_results_df[nowcast_results_df.index >= five_years_ago]

plt.figure(figsize=(12, 6))
plt.plot(filtered_results.index, filtered_results['actual'], label="Actual GDP Growth")
plt.plot(filtered_results.index, filtered_results['predicted'], label="Predicted GDP Growth")
plt.legend()
plt.title("Nowcast vs Actual GDP Growth (Last 5 Years)")
plt.xlabel("Date")
plt.ylabel("GDP Growth")
plt.grid(True)
plt.show()

"""### **Random Forest Regression - Trained on Last 5 Years of Data**"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error
import numpy as np
import pandas as pd

# Ensure df has the latest lagged and growth features
df['GDP_lag1'] = df['GDP'].shift(1)
df['GDP_growth'] = df['GDP'].pct_change()

# Drop rows with NaN values that result from lagging/pct_change for the full dataset
df_model_full = df.dropna(subset=['GDP_lag1', 'GDP_growth', 'hospitality_index'])

# Filter df_model for only the last five years for training
five_years_ago_train = df_model_full.index.max() - pd.DateOffset(years=5)
df_model_5_years_train = df_model_full[df_model_full.index >= five_years_ago_train]

X_5_years_train = df_model_5_years_train[['GDP_lag1', 'hospitality_index']]
y_5_years_train = df_model_5_years_train['GDP_growth']

tscv_5_years = TimeSeriesSplit(n_splits=3) # Reduced splits for potentially smaller dataset

all_preds_5_years = []
all_actuals_5_years = []
all_dates_5_years = []

for train_idx, test_idx in tscv_5_years.split(X_5_years_train):
    X_train_5_years, X_test_5_years = X_5_years_train.iloc[train_idx], X_5_years_train.iloc[test_idx]
    y_train_5_years, y_test_5_years = y_5_years_train.iloc[train_idx], y_5_years_train.iloc[test_idx]

    rf_5_years = RandomForestRegressor(n_estimators=500, random_state=42)
    rf_5_years.fit(X_train_5_years, y_train_5_years)

    pred_5_years = rf_5_years.predict(X_test_5_years)
    all_preds_5_years.extend(pred_5_years)
    all_actuals_5_years.extend(y_test_5_years.tolist())
    all_dates_5_years.extend(y_test_5_years.index.tolist())

# Create a DataFrame for results specific to this 5-year training
nowcast_results_df_5_years_trained = pd.DataFrame({
    'date': all_dates_5_years,
    'actual': all_actuals_5_years,
    'predicted': all_preds_5_years
}).set_index('date').sort_index()

rmse_5_years_trained = np.sqrt(mean_squared_error(all_actuals_5_years, all_preds_5_years))
print("RF RMSE (Model Trained Only on Last 5 Years Data):", rmse_5_years_trained)

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.plot(nowcast_results_df_5_years_trained.index, nowcast_results_df_5_years_trained['actual'], label="Actual GDP Growth")
plt.plot(nowcast_results_df_5_years_trained.index, nowcast_results_df_5_years_trained['predicted'], label="Predicted GDP Growth")
plt.legend()
plt.title("Nowcast vs Actual GDP Growth (Model Trained on Last 5 Years)")
plt.xlabel("Date")
plt.ylabel("GDP Growth")
plt.grid(True)
plt.show()

df['hospitality_lag1'] = df['hospitality_index'].shift(1)
df['hospitality_lag2'] = df['hospitality_index'].shift(2)

df['trend_momentum'] = df['hospitality_index'].diff()

df['GDP_growth_lag1'] = df['GDP_growth'].shift(1)

"""###**Save Nowcasting results**"""

import os

results_folder = os.path.join(project_root, 'results')
os.makedirs(results_folder, exist_ok=True)

# Save the full dataset nowcasting results
nowcast_full_path = os.path.join(results_folder, 'nowcast_results_full_dataset.csv')
nowcast_results_df.to_csv(nowcast_full_path)
print(f"Full dataset nowcasting results saved to: {nowcast_full_path}")

# Save the nowcasting results from the model trained on the last five years
nowcast_5yr_trained_path = os.path.join(results_folder, 'nowcast_results_5_years_trained.csv')
nowcast_results_df_5_years_trained.to_csv(nowcast_5yr_trained_path)
print(f"Nowcasting results (model trained on last 5 years) saved to: {nowcast_5yr_trained_path}")